version: "3.8"

services:
#  zookeeper:
#    image: confluentinc/cp-zookeeper:7.4.0
#    container_name: smoke_zookeeper
#    environment:
#      ZOOKEEPER_CLIENT_PORT: ${ZOOKEEPER_CLIENT_PORT}
#      ZOOKEEPER_TICK_TIME: 2000  # 2 seconds
#    ports:
#      - "${ZOOKEEPER_CLIENT_PORT}:${ZOOKEEPER_CLIENT_PORT}"
#
#  kafka:
#    image: confluentinc/cp-kafka:7.4.0
#    container_name: smoke_kafka
#    depends_on:
#      - zookeeper
#    ports:
#      - "${KAFKA_PORT}:${KAFKA_PORT}"
#    environment:
#      KAFKA_BROKER_ID: ${KAFKA_BROKER_ID}
#      KAFKA_ZOOKEEPER_CONNECT: zookeeper:${ZOOKEEPER_CLIENT_PORT}
#      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:${KAFKA_PORT}
#      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:${KAFKA_PORT}
#      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
#      KAFKA_CREATE_TOPICS: ${KAFKA_TOPIC_SMOKE}:1:1   # topic: partitions: replicas
#      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true" # 1 partition and 1 relica by default; need to set it false later
#
#  smoke_kafka_producer:
#    build:
#      context: .
#      dockerfile: data_ingestion/Dockerfile
#    container_name: kafka_producer
#    env_file:
#      - .env
#    depends_on:
#      - kafka
#    environment:
#      KAFKA_BOOTSTRAP_SERVERS: kafka:${KAFKA_PORT}
#    command: >
#      sh -c "sleep 10 &&
#           python data_ingestion/stream/kafka_producer.py"
#    volumes:
#      - ./data:/app/data
#    restart: unless-stopped

#  smoke_kafka_consumer_test:
#    build:
#      context: .
#      dockerfile: data_ingestion/Dockerfile
#    container_name: kafka_consumer
#    depends_on:
#      - smoke_kafka
#    environment:
#      KAFKA_BOOTSTRAP_SERVERS: kafka:${KAFKA_PORT}
#    command: ["python", "stream/kafka_consumer_test.py"]
#    volumes:
#      - ./data:/app/data
#    restart: unless-stopped
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "${POSTGRES_PORT}:${POSTGRES_PORT}"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  airflow:
    build:
      context: .
      dockerfile: data_processing/batch_processing/Dockerfile
    container_name: airflow
    depends_on:
      - postgres  
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'True'
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:${POSTGRES_PORT}/airflow
      AIRFLOW__WEBSERVER__WEB_SERVER_PORT: 8080
      AIRFLOW_UID: 50000
      AIRFLOW_GID: 50000
    ports:
      - "8080:8080"
    volumes:
      - ./data_processing/batch_processing/dags:/opt/airflow/dags
      - ./data_processing/batch_processing/tasks:/opt/airflow/tasks
      - ./data:/opt/airflow/data
      - ./app:/opt/airflow/app
      - ./config:/opt/airflow/config
      - ./data_ingestion:/opt/airflow/data_ingestion
    command: >
      bash -c "
      airflow db upgrade &&
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin &&
      airflow scheduler &
      airflow webserver
      "
    restart: unless-stopped

#  flask_api:
#    build:
#      context: ./frontend
#      dockerfile: Dockerfile
#    ports:
#      - "${FLASK_PORT}:${FLASK_PORT}"
#    env_file:
#      - .env
#    depends_on:
#      - kafka
#
#  prometheus:
#    image: prom/prometheus
#    ports:
#      - "${PROMETHEUS_PORT}:9090"
#    volumes:
#      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
#
#  grafana:
#    image: grafana/grafana
#    ports:
#      - "${GRAFANA_PORT}:3000"
#    volumes:
#      - ./monitoring/grafana:/var/lib/grafana

volumes:
  postgres_data: